# Описание тестового задания

1. Допишите часть кода, чтобы зависимость get_pg_connection действительно
   возвращала подключение к PostgreSQL в обработчике get_db_version.
   - Запрещено использовать глобальные переменные и объекты, а также любые
     механизмы, которые сохраняют данные вне локального контекста функций или
     методов
   - Необходимо использовать пул подключений, который предоставляет библиотека
     asyncpg
   - Запрещено использовать устаревшие (с пометкой deprecated) фичи FastAPI
2. Определите метод get_repositories в классе GithubReposScrapper, который
   возвращает Data-класс Repository, где authors_commits_num_today - список
   авторов с количеством коммитов за последний день.
   - Отправлять запросы для получения коммитов репозитория АСИНХРОННО
   - Ограничить максимальное количество одновременных запросов (MCR)
   - Ограничить количество запросов в секунду (RPS)
3. На базе кода из 2-го задания реализуйте сохранение данных в ClickHouse о
   репозитории, его позиции в топе и количество коммитов авторов (схемы таблиц
   уже определены в приложении к заданию).
   - Необходимо использовать библиотеку aiochclient
   - Эффективно использовать оперативную память с помощью вставки данных батчами
4. Напишите SQL-запрос в ClickHouse, который достаёт из заданной таблицы по
   определенной рекламной кампании просмотры по поисковым запросам по часам за
   сегодня. Формат ответа должен быть следующим:

   ```
   phrase    views_by_hour
   платье   [(15, 4), (14, 6), (13, 4), (12, 1)]
   ```

Другие требования:

- Решение должно быть выложено на GitHub - одно репо, разбитое на фолдеры
  аналогично задачам
- Не объединять задачи в монолитное приложение, каждая задача (кроме 3)
  выполняется независимо от других
- Python 3.11+, PEP8
- Код должен быть безопасным, с обработчиками ошибок
- Значительное внимание уделить архитектуре и использовать современные решения
- Не использовать хардкод, вместо этого применять конфигурационные файлы,
  переменные окружения или другие механизмы настройки
- В целом это должен быть production-ready код, в том виде, в котором вы бы его
  рекомендовали к релизу

# Настройка и Запуск

**Важно** в 1-3 проектах используются `.env` файлы для задания переменных
окружения. Перед выполнением каких-либо действий нужно скопировать шаблон
(`.env.template`) лежащий в соответствующей директории и поменять переменные
окружения на те что соответствуют системе в которой запущен проект (во 2 и 3 без
этого не будет задан токен для github, из-за чего парсер не сможет корректно
отправлять запросы).

```sh
cp .env.template .env
$EDITOR .env
```

Также рядом с `.env` файлами лежат `.envrc` которые позволяют автоматически
применять переменные окружения из `.env` и виртуальную среду из менеджера
проектов [uv](https://github.com/astral-sh/uv) с помощью
[direnv](https://github.com/direnv/direnv). Это не обязательный инструмент, но
позволяет упростить управлением окружением.

Ниже идут пункты описывающие запуск каждого из проектов:

## 1

Для запуска проекта с минимальной настройкой можно использовать `docker` и
`docker-compose` (либо установить `compose` в виде отдельного плагина для
докера):

```sh
docker-compose up --build
```

После этого можно обратится к эндпоинту используя обычный запрос в браузере или
через терминал с помощью `curl`:

```sh
curl http://localhost:5432/api/db_version
```

БД в `compose.yaml` написан для упрощения разворачивания и плохо подходит для
продакшена. Рассчитывается что в продакшене будет использоваться отдельный
сервер под БД, адрес которой будет задаваться через переменную окружения
`DB_URL`.

Проект также можно запустить без докера, с помощью менеджера проектов `uv`. Для
этого достаточно вызвать `uv run` передав ему файл окружения:

```sh
uv run --env-file=.env main.py
```

PostgreSQL при этом должен быть настроен локально подробнее можно почитать в
документации:
[installation](https://www.postgresql.org/docs/current/tutorial-install.html)

## 2

Проект запускается с помощью `uv`, результат пишется в stdout (т.е. его можно
перенаправить в файл при необходимости):

```sh
uv run --env-file=.env main.py > scrape-result.txt
```

## 3

Также как и 1 проект, можно поднять с помощью `docker` и `docker-compose`:

```sh
docker-compose up --build
```

Результат будет находится в clickhouse, который доступен по портам по умолчанию
(8123 http, 9000 tcp).

Также доступен запуск с помощью `uv`:

```sh
uv run --env-file=.env src/main.py
```

ClickHouse При этом должен быть настроен локально, подробнее можно почитать в
документации: [Install ClickHouse](https://clickhouse.com/docs/install)

## 4

Запрос находится в `query.sql`, параметры заданы через `with` в самом начале
(поскольку для текущего дня там данных нет, то вариант с использованием `now`
закомментирован)
